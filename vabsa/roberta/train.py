# -*- coding: utf-8 -*-
"""ABSA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15x2MK0REqa-jGjYmlKFzWydDLkWUlMtD
"""

import pandas as pd
import torch
from transformers import BertTokenizer
base_dir = "roberta/"
import numpy as np
from scipy.special import softmax
import evaluation
from transformers import AutoTokenizer
train = pd.read_csv("datasets/preprocessed/roberta/prepared_train.csv")
valid = pd.read_csv("datasets/preprocessed/roberta/prepared_valid.csv")
test = pd.read_csv("datasets/preprocessed/roberta/prepared_test.csv")
num_class = 4

test.info()

train.head()

"""### Get data for model"""

def get_dataset(df_file_csv):
    original_sentences = []
    auxiliary_sentences = []
    labels = []
    aspect_dict = {"AMBIENCE#GENERAL" : "nhận xét chung ngoại cảnh","DRINKS#PRICES": "giá đồ uống", "DRINKS#QUALITY": "chất lượng đồ uống", 
                   "DRINKS#STYLE&OPTIONS": "các lựa chọn và phong cách đồ uống" , "FOOD#PRICES": "giá đồ ăn","FOOD#QUALITY": "chất lượng đồ ăn","FOOD#STYLE&OPTIONS": "các lựa chọn và phong cách đồ ăn","LOCATION#GENERAL": "nhận xét chung vị trí",
                   "RESTAURANT#GENERAL": "nhận xét chung về nhà hàng", "RESTAURANT#MISCELLANEOUS": "khía cạnh khác của nhà hàng","RESTAURANT#PRICES": "giá nhà hàng","SERVICE#GENERAL": "nhận xét chung dịch vụ"}
    for row in range(len(df_file_csv)):
        original_sentences.append(str(df_file_csv.loc[row, "sentence"]))
        auxiliary_sentences.append(str(aspect_dict[df_file_csv.loc[row, "aspect"]]))
        labels.append((df_file_csv.loc[row, "label_id"]))
    return original_sentences, auxiliary_sentences, labels

train_original_sentences, train_auxiliary_sentences, train_labels = get_dataset(train)
val_original_sentences, val_auxiliary_sentences, val_labels = get_dataset(valid)
test_original_sentences, test_auxiliary_sentences, test_labels = get_dataset(test)

PRETRAINED_MODEL = 'vinai/phobert-base'

tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)

train_encodings = tokenizer(train_original_sentences, train_auxiliary_sentences, truncation=True, padding=True)
val_encodings = tokenizer(val_original_sentences, val_auxiliary_sentences, truncation=True, padding=True)
test_encodings = tokenizer(test_original_sentences, test_auxiliary_sentences, truncation=True, padding=True)

class ABSA_Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = ABSA_Dataset(train_encodings, train_labels)
val_dataset = ABSA_Dataset(val_encodings, val_labels)
test_dataset = ABSA_Dataset(test_encodings, test_labels)

def get_test_labels(df_test_csv):
    original_sentences = []
    auxiliary_sentences = []
    labels = []
    for row in range(len(df_test_csv)):
        labels.append(df_test_csv.loc[row, "label_id"])
    return labels

def get_predictions(data):
    predicted_labels = []
    scores = []
    count_aspect_rows = 0
    current_aspect_scores = []
    for row in data:
        current_aspect_scores.append(row[2])
        count_aspect_rows += 1
        if count_aspect_rows % 12 == 0:
            sum_current_aspect_scores = np.sum(current_aspect_scores)
            current_aspect_scores = [score / sum_current_aspect_scores for score in current_aspect_scores]
            scores.append(current_aspect_scores)
            predicted_labels.append(np.argmax(current_aspect_scores))
            current_aspect_scores = []
    return predicted_labels, scores

def compute_metrics(predictions):
    df_csv = pd.read_csv("datasets/preprocessed/roberta/prepared_valid.csv")
    scores = [softmax(prediction) for prediction in predictions[0]]
    predicted_labels = [np.argmax(x) for x in scores]
    data = np.insert(scores, 0, predicted_labels, axis=1)
    # predicted_labels, scores = get_predictions(data)

    test_labels = get_test_labels(df_csv)
    metrics = {}
    p, r, f1 = evaluation.compute_semeval_PRF(test_labels, predicted_labels)
    metrics["P"] = p
    metrics["R"] = r
    metrics["F1"] = f1
    metrics["F1-pair"] = evaluation.compute_f1_aspect_sentiment(test_labels, predicted_labels, scores)
    return metrics

from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments, BertConfig

from transformers import logging
logging.set_verbosity_debug()


epochs = 15
num_classes = 4
batch_size = 16
num_steps = len(train_dataset) * epochs // batch_size
warmup_steps = num_steps // 10  # 10% of the training steps
save_steps = 2221 # Save a checkpoint at the end of each epoch
eval_steps= 2221


training_args = TrainingArguments(
    output_dir = f'{base_dir}/model_roberta_tv/',
    num_train_epochs = epochs,
    per_device_train_batch_size = batch_size,
    per_device_eval_batch_size = batch_size,
    warmup_steps = warmup_steps,
    weight_decay = 0.01,
    logging_dir = f'{base_dir}/logs/',
    logging_steps = 2221,
    evaluation_strategy = 'steps',
    learning_rate = 2e-5,
    save_steps = save_steps,
    eval_steps = eval_steps,
    eval_accumulation_steps=1,
    report_to='tensorboard'
)

config = BertConfig.from_pretrained(
    PRETRAINED_MODEL,
    architectures = ['RobertaForSequenceClassification'],
    hidden_size = 768,
    num_hidden_layers = 12,
    num_attention_heads = 12,
    hidden_dropout_prob = 0.1,
    num_labels = num_classes
    
)

load_finetuned_model = False
if not load_finetuned_model:
    model = RobertaForSequenceClassification.from_pretrained(PRETRAINED_MODEL, config=config)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
        
    )
    trainer.train()

    model.save_pretrained(f"{base_dir}/models/BERT-pair-tv/last_step")

else:
    model = RobertaForSequenceClassification.from_pretrained(f"{base_dir}/models/BERT-pair-tv/last_step")

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
    )

