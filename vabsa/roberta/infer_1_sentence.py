from transformers import AutoTokenizer
from transformers import RobertaForSequenceClassification
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report
from tqdm import tqdm
import torch
import warnings
import string
warnings.filterwarnings("ignore")
import re

device = 'cuda'

model = RobertaForSequenceClassification.from_pretrained("checkpoints\\roberta", num_labels=4).to(device).eval()
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")

def normalize_money(sent):
    return re.sub(r'[0-9]+[.,0-9][k-m-b]', 'giรก', sent)

def normalize_hastag(sent):
    return re.sub(r'#+\w+', 'tag', sent)

def normalize_website(sent):
    result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'website', sent)
    return re.sub(r'\w+(\.(com|vn|me))+((\/+([\.\w\_\-]+)?)+)?', 'website', result)

def nomalize_emoji(sent):
    emoji_pattern = re.compile("["
                u"\U0001F600-\U0001F64F"  # emoticons
                u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                u"\U0001F680-\U0001F6FF"  # transport & map symbols
                u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                u"\U00002702-\U000027B0"
                u"\U000024C2-\U0001F251"
                u"\U0001f926-\U0001f937"
                u'\U00010000-\U0010ffff'
                u"\u200d"
                u"\u2640-\u2642"
                u"\u2600-\u2B55"
                u"\u23cf"
                u"\u23e9"
                u"\u231a"
                u"\u3030"
                u"\ufe0f"
    "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', sent)

def normalize_elongate(sent):
    patern = r'(.)\1{1,}'
    result = sent
    while(re.search(patern, result) != None):
        repeat_char = re.search(patern, result)
        result = result.replace(repeat_char[0], repeat_char[1])
    return result

def remove_number(sent):
    return re.sub(r'[0-9]+', '', sent)

def normalize_acronyms(sent):
    text = sent
    replace_list = {
        'รด kรชi': ' ok ', 'okie': ' ok ', ' o kรช ': ' ok ',
        'okey': ' ok ', 'รดkรช': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okรช':' ok ',
        ' tks ': u' cรกm ฦกn ', 'thks': u' cรกm ฦกn ', 'thanks': u' cรกm ฦกn ', 'ths': u' cรกm ฦกn ', 'thank': u' cรกm ฦกn ',
        'โญ': 'star ', '*': 'star ', '๐': 'star ', '๐': u' tรญch cแปฑc ',
        'kg ': u' khรดng ','not': u' khรดng ', u' kg ': u' khรดng ', '"k ': u' khรดng ',' kh ':u' khรดng ','kรด':u' khรดng ','hok':u' khรดng ',' kp ': u' khรดng phแบฃi ',u' kรด ': u' khรดng ', '"ko ': u' khรดng ', u' ko ': u' khรดng ', u' k ': u' khรดng ', 'khong': u' khรดng ', u' hok ': u' khรดng ',
        'he he': ' tรญch cแปฑc ','hehe': ' tรญch cแปฑc ','hihi': ' tรญch cแปฑc ', 'haha': ' tรญch cแปฑc ', 'hjhj': ' tรญch cแปฑc ',
        ' lol ': ' tiรชu cแปฑc ',' cc ': ' tiรชu cแปฑc ','cute': u' dแป thฦฐฦกng ','huhu': ' tiรชu cแปฑc ', ' vs ': u' vแปi ', 'wa': ' quรก ', 'wรก': u' quรก', 'j': u' gรฌ ', 'โ': ' ',
        ' sz ': u' cแปก ', 'size': u' cแปก ', u' ฤx ': u' ฤฦฐแปฃc ', 'dk': u' ฤฦฐแปฃc ', 'dc': u' ฤฦฐแปฃc ', 'ฤk': u' ฤฦฐแปฃc ',
        'ฤc': u' ฤฦฐแปฃc ','authentic': u' chuแบฉn chรญnh hรฃng ',u' aut ': u' chuแบฉn chรญnh hรฃng ', u' auth ': u' chuแบฉn chรญnh hรฃng ', 'thick': u' tรญch cแปฑc ', 'store': u' cแปญa hรng ',
        'shop': u' cแปญa hรng ', 'sp': u' sแบฃn phแบฉm ', 'gud': u' tแปt ','god': u' tแปt ','wel done':' tแปt ', 'good': u' tแปt ', 'gรบt': u' tแปt ',
        'sแบฅu': u' xแบฅu ','gut': u' tแปt ', u' tot ': u' tแปt ', u' nice ': u' tแปt ', 'perfect': 'rแบฅt tแปt', 'bt': u' bรฌnh thฦฐแปng ',
        'time': u' thแปi gian ', 'qรก': u' quรก ', u' ship ': u' giao hรng ', u' m ': u' mรฌnh ', u' mik ': u' mรฌnh ',
        'รชฬ': 'แป', 'product': 'sแบฃn phแบฉm', 'quality': 'chแบฅt lฦฐแปฃng','chat':' chแบฅt ', 'excelent': 'hoรn hแบฃo', 'bad': 'tแป','fresh': ' tฦฐฦกi ','sad': ' tแป ',
        'date': u' hแบกn sแปญ dแปฅng ', 'hsd': u' hแบกn sแปญ dแปฅng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hรng ',u' sรญp ': u' giao hรng ',
        'beautiful': u' ฤแบนp tuyแปt vแปi ', u' tl ': u' trแบฃ lแปi ', u' r ': u' rแปi ', u' shopE ': u' cแปญa hรng ',u' order ': u' ฤแบทt hรng ',
        'chแบฅt lg': u' chแบฅt lฦฐแปฃng ',u' sd ': u' sแปญ dแปฅng ',u' dt ': u' ฤiแปn thoแบกi ',u' nt ': u' nhแบฏn tin ',u' tl ': u' trแบฃ lแปi ',u' sรi ': u' xรi ',u'bjo':u' bao giแป ',
        'thik': u' thรญch ',u' sop ': u' cแปญa hรng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rแบฅt ',u'quแบฃ ng ':u' quแบฃng  ',
        'dep': u' ฤแบนp ',u' xau ': u' xแบฅu ','delicious': u' ngon ', u'hรg': u' hรng ', u'qแปงa': u' quแบฃ ',
        'iu': u' yรชu ','fake': u' giแบฃ mแบกo ', 'trl': 'trแบฃ lแปi', '><': u' tรญch cแปฑc ',
        ' por ': u' tแป ',' poor ': u' tแป ', 'ib':u' nhแบฏn tin ', 'rep':u' trแบฃ lแปi ',u'fback':' feedback ','fedback':' feedback '
    }
    for k, v in replace_list.items():
        text = text.replace(k, v)
    return text


def normalize(sent):
    result = normalize_money(sent)
    result = normalize_hastag(result)
    result = normalize_website(result)
    result = nomalize_emoji(result)
    result = normalize_elongate(result)
    result = normalize_acronyms(result)
    result = remove_number(result)
    result = result.lower()
    return result.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))).replace(' '*4, ' ').replace(' '*3, ' ').replace(' '*2, ' ').strip()
def preprocess_data(sent):
    sent = normalize(sent)   
    return sent 

predictions = []

aspect_dict = {"AMBIENCE#GENERAL" : "nhแบญn xรฉt chung ngoแบกi cแบฃnh","DRINKS#PRICES": "giรก ฤแป uแปng", "DRINKS#QUALITY": "chแบฅt lฦฐแปฃng ฤแป uแปng", 
                   "DRINKS#STYLE&OPTIONS": "cรกc lแปฑa chแปn vร phong cรกch ฤแป uแปng" , "FOOD#PRICES": "giรก ฤแป ฤn","FOOD#QUALITY": "chแบฅt lฦฐแปฃng ฤแป ฤn","FOOD#STYLE&OPTIONS": "cรกc lแปฑa chแปn vร phong cรกch ฤแป ฤn","LOCATION#GENERAL": "nhแบญn xรฉt chung vแป trรญ",
                   "RESTAURANT#GENERAL": "nhแบญn xรฉt chung vแป nhร hรng", "RESTAURANT#MISCELLANEOUS": "khรญa cแบกnh khรกc cแปงa nhร hรng","RESTAURANT#PRICES": "giรก nhร hรng","SERVICE#GENERAL": "nhแบญn xรฉt chung dแปch vแปฅ"}

test_sentence = "ฤรขy lร trong nhแปฏng quรกn mร mรฌnh thรญch vรฌ vแป trร ฤแบญm vร thฦกm cลฉng nhฦฐ mรนi vแป ฤแบทc trฦฐng hฦกn hแบณn nhแปฏng quรกn khรกc nรจ trร sแปฏa trรขn chรขu sแปฃi giรก trร sแปฏa pha khรก ngon vแป trร chรกt vร mรนi hฦฐฦกng khรก rรต khรดng quรก ngแปt rแบฅt ฤรบng vแปi gu mรฌnh trร ฤรo giรก vแป trร ฤรo แป ฤรขy cลฉng ฤแบทc biแปt hฦกn hแบณn nhแปฏng quรกn khรกc khรดng phแบฃi chua ngแปt nhฦฐ thฦฐแปng thแบฅy mร cรณ mรนi trร rแบฅt ngon cร phรช ฤรก xay giรก mรณn ฤรก xay แป ฤรขy uแปng cลฉng ngon khรดng kรฉm trร nรจ mรนi vแป thฦกm hฦฐฦกng cร phรช vแป ฤแบฏng kแบฟt hแปฃp hoรn hแบฃo vแปi ฤแป bรฉo ngแปt cแปงa whiping cream khรดng quรก ฤแบฏng cลฉng khรดng quรก ngแปt hay lแบกt lแบฝo mร dแปu nhแบน thฦกm vร dแป uแปng lแบฏm trร vแบฃi thiแบฟt quan รขm giรก trร vแบฃi cรณ mรนi vแป rแบฅt thฦกm ngon mรนi vแบฃi mร vแบซn nghe rรต vแป trร cรณ chรบt vแป chรกt nhแบน mรนi trร thฦกm rแบฅt thรญch khรดng phแบฃi chแป toรn vแป syrup vแบฃi ngแปt gแบฏt nhฦฐ nhiแปu chแป khรกc do trร แป ฤรขy pha khรก ฤแบญm nรชn bแบกn nรo uแปng mร ฤang ฤรณi sแบฝ dแป say nha hoแบทc ban ฤรชm cรณ thแป khรณ ngแปง ร cแบฃnh bรกo trฦฐแปc trร thiแบฟt quan รขm late giรก ly nรy thรฌ vแป trร rแบฅt ฤแบญm nรชn cแบฃm giรกc hฦกi nhแบกt vร chรกt phแบงn kem sแปฏa bรชn trรชn bรฉo bรฉo uแปng chung thรฌ vแปซa trร sแปฏa giรก ly nรy chแปฅp kรฉ chแปฉ khรดng cรณ thแปญ v"
test_sentence = preprocess_data(test_sentence)

polarity_dict = {0: "none", 1: "positive", 2: "negative", 3: "neutral"}
for i in tqdm(aspect_dict.keys()):

    test_encodings = tokenizer(test_sentence, str(aspect_dict[i]),truncation=True, padding=True, return_tensors='pt').input_ids.to(device)
    logits = model(test_encodings).logits

    prediction = np.argmax(logits.cpu().detach().numpy(), axis=-1)[0]
    if int(prediction) != 0:
       predictions.append(str(i) + " - " + str(polarity_dict[int(prediction)]))
    else:
        continue

print("Testing Sentence: ", test_sentence)
print("Prediction: ", ', '.join(str(x) for x in predictions))