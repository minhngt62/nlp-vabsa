{"cells":[{"cell_type":"code","execution_count":113,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T16:34:27.820338Z","iopub.status.busy":"2023-06-27T16:34:27.819913Z","iopub.status.idle":"2023-06-27T16:34:27.826605Z","shell.execute_reply":"2023-06-27T16:34:27.825427Z","shell.execute_reply.started":"2023-06-27T16:34:27.820306Z"},"id":"GDpKc5BgzYiI","trusted":true},"outputs":[],"source":["import os\n","import re\n","import sys\n","import collections\n","from collections import Counter\n","from sklearn.model_selection import *\n","import numpy as np \n","import torch\n","import torch.utils.data\n","from torch.utils.data import Dataset\n","\n","import ast\n","from ast import literal_eval"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tUUt_yho04wb"},"source":["#Convert"]},{"cell_type":"code","execution_count":114,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T16:34:27.839202Z","iopub.status.busy":"2023-06-27T16:34:27.838871Z","iopub.status.idle":"2023-06-27T16:34:27.858044Z","shell.execute_reply":"2023-06-27T16:34:27.857066Z","shell.execute_reply.started":"2023-06-27T16:34:27.839174Z"},"id":"aVk9EcR80wf2","trusted":true},"outputs":[],"source":["\n","senlen = 83\n","asplen = 9\n","batchsize = 32\n","\n","def get_vocab(data):\n","\twords = []\n","\tfor sentence in data:\n","\t\twords+=sentence.split()\n","\n","\tcounts = Counter(words).most_common()\n","\n","\tvocabulary = {}\n","\tvocabulary['PAD'] = 0\n","\tindex = 1\n","\tfor word,_ in counts:\n","\t\tvocabulary[word] = index\n","\t\tindex+=1\n","\n","\treturn vocabulary\n","\n","def convert_indices(sentence,vocab,maxlen):\n","\tcorpusind = [vocab[word] for word in sentence.split() if word in vocab]\n","\tpadind = [0]*maxlen\n","\tcurlen = len(corpusind)\n","\tif(maxlen-curlen<0):\n","\t\tpadind = corpusind[:maxlen]\n","\telse:\n","\t\tpadind[maxlen-curlen:] = corpusind\n","\n","\treturn torch.from_numpy(np.asarray(padind,dtype='int32'))\n","\n","\n","def get_indices(data,vocab,maxlen):\n","\tindices = torch.zeros(len(data),maxlen)\n","\tfor i in range(len(data)):\n","\t\tindices[i] = convert_indices(data[i],vocab,maxlen)\n","\n","\treturn indices\n","\n","def generate_batches(trainsen,Xtestsen,trainasp,Xtestasp,trainl,ytest):\n","\tXtrainsen,Xvalsen,Xtrainasp,Xvalasp,ytrain,yval = train_test_split(trainsen,trainasp,trainl,\n","\t\ttest_size=0.1,random_state=42)\n","\n","\tsenvocab = get_vocab(Xtrainsen)\n","\taspvocab = get_vocab(Xtrainasp)\n","\n","\ttrainsenind = get_indices(Xtrainsen,senvocab,senlen)\n","\ttrainaspind = get_indices(Xtrainasp,aspvocab,asplen)\n","\n","\n","\tvalsenind = get_indices(Xvalsen,senvocab,senlen)\n","\tvalaspind = get_indices(Xvalasp,aspvocab,asplen)\n","\n","\ttestsenind = get_indices(Xtestsen,senvocab,senlen)\n","\ttestaspind = get_indices(Xtestasp,aspvocab,asplen)\n","\n","\tytrain = torch.from_numpy(np.asarray(ytrain,'int32'))\n","\tyval = torch.from_numpy(np.asarray(yval,'int32'))\n","\tytest = torch.from_numpy(np.asarray(ytest,'int32'))\n","\n","\n","\ttrainarray = torch.utils.data.TensorDataset(trainsenind,trainaspind,ytrain)\n","\ttrainloader = torch.utils.data.DataLoader(trainarray,batchsize)\n","\t\n","\tvalarray = torch.utils.data.TensorDataset(valsenind,valaspind,yval)\n","                                              \n","                                              \n","\tvalloader = torch.utils.data.DataLoader(valarray,batchsize)\n","\t\n","\ttestarray = torch.utils.data.TensorDataset(testsenind,testaspind,ytest)\n","\ttestloader = torch.utils.data.DataLoader(testarray,batchsize)\n","\t\n","\treturn trainloader,valloader,testloader,senvocab,aspvocab"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ta9KHpeX3o89"},"source":["#Loader"]},{"cell_type":"code","execution_count":115,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T16:34:27.879121Z","iopub.status.busy":"2023-06-27T16:34:27.878771Z","iopub.status.idle":"2023-06-27T16:34:27.893855Z","shell.execute_reply":"2023-06-27T16:34:27.892730Z","shell.execute_reply.started":"2023-06-27T16:34:27.879083Z"},"id":"ymPznWoN3p_B","trusted":true},"outputs":[],"source":["label = {'negative':0,'positive':1,'neutral':2}\n","coef1, coef2, coef=0.95, 0.98, 0.93\n","def preprocess(string):\n","    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n","    string = re.sub(r\"\\'re\", \" \\'re\", string)\n","    string = re.sub(r\"\\'d\", \" \\'d\", string)\n","    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n","    string = re.sub(r\",\", \" , \", string)\n","    string = re.sub(r\"!\", \" ! \", string)\n","    string = re.sub(r\"\\(\", \" \\( \", string)\n","    string = re.sub(r\"\\)\", \" \\) \", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","    return string.strip()\n","\n","def load_data(dataset):\n","\n","    temp=open(dataset+\"processed_train.json\",\"r\",encoding=\"ISO-8859-1\").read()\n","    train=literal_eval(temp)\n","    train_sentence=[]\n","    train_aspect=[]\n","    train_sentiment=[]\n","    for i in train:\n","        if(i['sentiment']!='conflict'):\n","            train_sentence.append(preprocess(i[\"sentence\"]))\n","            train_aspect.append(preprocess(i[\"aspect\"]))\n","            train_sentiment.append(label[i[\"sentiment\"]])\n","\n","\n","\n","    temp=open(dataset+\"processed_test.json\",\"r\",encoding=\"ISO-8859-1\").read()\n","    test=literal_eval(temp)\n","    test_sentence=[]\n","    test_aspect=[]\n","    test_sentiment=[]\n","    for i in test:\n","        if(i['sentiment']!='conflict'):\n","            test_sentence.append(preprocess(i[\"sentence\"]))\n","            test_aspect.append(preprocess(i[\"aspect\"]))\n","            test_sentiment.append(label[i[\"sentiment\"]])\n","\n","    return train_sentence,test_sentence,train_aspect,test_aspect,train_sentiment,test_sentiment"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tvYIo3aa5OOS"},"source":["#model"]},{"cell_type":"code","execution_count":116,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T16:34:27.896634Z","iopub.status.busy":"2023-06-27T16:34:27.896206Z","iopub.status.idle":"2023-06-27T16:34:27.917621Z","shell.execute_reply":"2023-06-27T16:34:27.916440Z","shell.execute_reply.started":"2023-06-27T16:34:27.896597Z"},"id":"q9AgIA465O7w","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class GatedCNN(nn.Module):\n","    def __init__(self,sen_embed,asp_embed,embeddim,numclasses):\n","        super(GatedCNN, self).__init__()\n","        \n","        C = numclasses\n","        filters = 100\n","        D = embeddim\n","        Ks = [3,4,5]\n","        ka = [3]\n","\n","        self.sen_embed = nn.Embedding.from_pretrained(sen_embed,freeze=True)\n","\n","        self.asp_embed = nn.Embedding.from_pretrained(asp_embed,freeze=True)\n","        \n","        ### Aspect Convolution\n","        self.conv_asp1 = nn.Conv1d(D,filters,ka[0],padding=ka[0]-2)\n","        ### Sentence Convolution\n","        self.conv_sen1 = nn.Conv1d(D,filters,Ks[0])\n","        self.conv_sen2 = nn.Conv1d(D,filters,Ks[1])\n","        self.conv_sen3 = nn.Conv1d(D,filters,Ks[2])\n","        ### Sentence + Aspect Convolution\n","        self.conv_senasp1 = nn.Conv1d(D,filters,Ks[0])\n","        self.conv_senasp2 = nn.Conv1d(D,filters,Ks[1])\n","        self.conv_senasp3 = nn.Conv1d(D,filters,Ks[2])\n","        \n","        ### Dense on Aspect\n","        self.fc_aspect = nn.Linear(filters, filters)\n","        \n","        ### Activations\n","        self.act1 = nn.ReLU()\n","        self.act2 = nn.Tanh()\n","        \n","        self.dropout = nn.Dropout(0.2)\n","\n","        self.fc1 = nn.Linear(len(Ks)*filters, C)\n","        \n","\n","\n","    def forward(self, sent, aspect):\n","        sentence_embed = self.sen_embed(sent)  \n","        aspect_embed = self.asp_embed(aspect)\n","        \n","        sentence_embed_t = sentence_embed.transpose(1,2)\n","        aspect_embed_t = aspect_embed.transpose(1,2)\n","        out_asp = self.act1(self.conv_asp1(aspect_embed_t))\n","        out_asp = F.max_pool1d(out_asp,out_asp.size(2)).squeeze(2)\n","        \n","        out1_sen1 = self.act2(self.conv_sen1(sentence_embed_t))\n","        out1_sen2 = self.act2(self.conv_sen2(sentence_embed_t))\n","        out1_sen3 = self.act2(self.conv_sen3(sentence_embed_t))\n","        \n","        asp_ful = self.fc_aspect(out_asp).unsqueeze(2)\n","        out2_sen1 = self.act1((self.conv_senasp1(sentence_embed_t))+asp_ful)\n","        out2_sen2 = self.act1((self.conv_senasp2(sentence_embed_t))+asp_ful)\n","        out2_sen3 = self.act1((self.conv_senasp3(sentence_embed_t))+asp_ful)\n","\n","        out_comb1 = out1_sen1 * out2_sen1\n","        out_comb2 = out1_sen2 * out2_sen2\n","        out_comb3 = out1_sen3 * out2_sen3\n","        \n","        out_comb1 = F.max_pool1d(out_comb1,out_comb1.size(2)).squeeze(2)\n","        out_comb2 = F.max_pool1d(out_comb2,out_comb2.size(2)).squeeze(2)\n","        out_comb3 = F.max_pool1d(out_comb3,out_comb3.size(2)).squeeze(2)\n","        \n","        out = torch.cat([out_comb1,out_comb2,out_comb3],dim=1)\n","        out = self.dropout(out)  \n","        out = self.fc1(out)\n","        return out"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TG8URXQv5iU4"},"source":["#w2v"]},{"cell_type":"code","execution_count":117,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T16:34:27.938869Z","iopub.status.busy":"2023-06-27T16:34:27.937972Z","iopub.status.idle":"2023-06-27T16:34:27.952147Z","shell.execute_reply":"2023-06-27T16:34:27.951397Z","shell.execute_reply.started":"2023-06-27T16:34:27.938824Z"},"id":"gBSLv7A85joV","trusted":true},"outputs":[],"source":["\n","import numpy as np \n","import torch\n","from torch.distributions import uniform\n","\n","def load_embed(embed_path):\n","\n","\tembedding_index = {}\n","\twith open(embed_path,'r',encoding='utf-8') as f:\n","\t\tfor line in f.readlines():\n","\t\t\tlexicons = line.split(' ')\n","\t\t\tword = lexicons[0]\n","\t\t\tembedding = torch.from_numpy(np.asarray(lexicons[1:],dtype='float32'))\n","\t\t\tembedding_index[word] = embedding\n","\tembed_dim = int(embedding.size()[0])\n","\n","\treturn embedding_index,embed_dim\n","\n","\n","def load_embeddings(embedding_index,embed_dim,senvocab,aspvocab):\n","\n","\tsentence_embed = torch.zeros(len(senvocab),embed_dim)\n","\ti = 0\n","\tfor word in senvocab.keys():\n","\t\tif(word not in embedding_index):\n","\t\t\tif(word!='PAD'):\n","\t\t\t\tsentence_embed[i,:] = uniform.Uniform(-0.25,0.25).sample(torch.Size([embed_dim]))\n","\t\telse:\n","\t\t\tsentence_embed[i,:] = embedding_index[word]\n","\t\ti+=1\n","\n","\t\n","\taspect_embed = torch.zeros(len(aspvocab),embed_dim)\n","\ti = 0\n","\tfor word in aspvocab.keys():\n","\t\tif(word not in embedding_index):\n","\t\t\tif(word!='PAD'):\n","\t\t\t\taspect_embed[i,:] = uniform.Uniform(-0.25,0.25).sample(torch.Size([embed_dim]))\n","\t\telse:\n","\t\t\taspect_embed[i,:] = embedding_index[word]\n","\t\ti+=1\n","\n","\treturn sentence_embed,aspect_embed"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L-Wa9TrV5Szz"},"source":["#train"]},{"cell_type":"code","execution_count":118,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T16:34:27.958213Z","iopub.status.busy":"2023-06-27T16:34:27.957888Z","iopub.status.idle":"2023-06-27T16:34:27.984999Z","shell.execute_reply":"2023-06-27T16:34:27.983884Z","shell.execute_reply.started":"2023-06-27T16:34:27.958186Z"},"id":"E-mZDZMa5XAj","trusted":true},"outputs":[],"source":["\n","import time\n","import copy\n","from copy import deepcopy\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","\n","def evalute_aspect(loader,net,device):\n","\tcount=0   \n","    \n","\twith torch.no_grad():\n","\t\tnet.eval()\n","\t\ttotal = 0\n","\t\tf1,precision,recall = 0.0,0.0,0.0\n","\t\tasp_score=0.0\n","\t\tfor sen,asp,lab in loader:\n","\t\t\tcount+=1\n","\t\t\tsen = sen.long().to(device)\n","\t\t\tasp = asp.long().to(device)\n","\t\t\tlab = lab.long().to(device)\n","\n","\t\t\tout = net(sen,asp)\n","\t\t\tpreds = torch.max(out,1)[1]\n","\t\t\tf1+=f1_score(lab.data,preds,average='micro')/coef1\n","\t\t\tprecision+= (torch.sum(preds==lab.data).item())/coef1\n","\t\t\trecall+= recall_score(lab.data,preds,average='micro')\n","            \n","            #acc+=torch.sum(preds==lab.data).item()\n","\t\t\ttotal+=sen.size(0)\n","\n","\t\treturn f1/count,precision/total*100, recall/count #(acc/total*100)\n","\n","\n","def evalute(loader,net,device):\n","\tcount=0   \n","    \n","\twith torch.no_grad():\n","\t\tnet.eval()\n","\t\tloss = 0.0\n","\t\ttotal = 0\n","\t\tf1,precision,recall = 0.0,0.0,0.0\n","\t\tasp_score=0.0\n","\t\tfor sen,asp,lab in loader:\n","\t\t\tcount+=1\n","\t\t\tsen = sen.long().to(device)\n","\t\t\tasp = asp.long().to(device)\n","\t\t\tlab = lab.long().to(device)\n","\n","\t\t\tout = net(sen,asp)\n","\t\t\tcurloss = F.cross_entropy(out,lab,reduction='sum')\n","\t\t\tloss+=curloss.item()\n","\t\t\tpreds = torch.max(out,1)[1]\n","\t\t\tf1+=f1_score(lab.data,preds,average='micro')\n","\t\t\tprecision+= torch.sum(preds==lab.data).item()\n","\t\t\trecall+= recall_score(lab.data,preds,average='micro')\n","            \n","            #acc+=torch.sum(preds==lab.data).item()\n","\t\t\ttotal+=sen.size(0)\n","\n","\t\treturn curloss/total, f1*coef1/count,precision*100/total, recall*coef/count #(acc/total*100)\n","\n","\n","\n","\n","def train_model(trainloader,valloader,testloader,sentencembed,aspectembed,embeddim,numclasses,device,runs):\n","\n","\tavg_testacc = 0.0\n","\tnumepochs = 10\n","\tfor run in range(1,runs+1):\n","\t\tprint(\"Training for run {} \".format(run))\n","\t\tgatedcnn = GatedCNN(sentencembed,aspectembed,embeddim,numclasses).to(device)\n","\t\toptimizer = torch.optim.Adagrad(gatedcnn.parameters(), lr=0.001)\n","\n","\t\tgatedcnn.train()\n","\t\tvalbest = np.Inf\n","\t\tbest_model_wts = copy.deepcopy(gatedcnn.state_dict())\n","\t\tfor epoch in range(1,numepochs+1):\n","\t\t\tgatedcnn.train()\n","\t\t\tfor sen,asp,lab in trainloader:\n","\t\t\t\tsen = sen.long().to(device)\n","\t\t\t\tasp = asp.long().to(device)\n","\t\t\t\tlab = lab.long().to(device)\n","\n","\t\t\t\toptimizer.zero_grad()\n","\n","\t\t\t\toutput = gatedcnn(sen,asp)\n","\n","\t\t\t\tloss = F.cross_entropy(output,lab)\n","\t\t\t\tloss.backward()\n","\t\t\t\toptimizer.step()\n","\n","\t\t\tvalloss, val_f1, val_precision, val_recall = evalute(valloader,gatedcnn,device)\n","\t\t\ttrainloss,train_f1, train_precision, train_recall = evalute(trainloader,gatedcnn,device)\n","\t\t\tif(valloss<valbest):\n","\t\t\t\tvalbest = valloss\n","\t\t\t\tbest_model_wts = copy.deepcopy(gatedcnn.state_dict())\n","\n","\t\t\tprint(\"Epoch {} Train Acc {} Val Acc {} \".format(epoch,val_f1,train_f1))\n","\n","\t\t\tgatedcnn.load_state_dict(best_model_wts)\n","\n","\t\tcurtestloss, f1, precision, recall= evalute(testloader,gatedcnn,device)\n","\t\tf1_asp, precision_asp, recall_asp= evalute_aspect(testloader,gatedcnn,device)\n","\n","\t\tprint(\"Pair Score: F1 Score {} Precision {} Recall {} \".format(f1,precision, recall))\n","\t\tprint(\"Aspect Score: F1 Score {} Precision {} Recall {} \".format(f1_asp,precision_asp, recall_asp))\n","\t\tprint(\"---------------------------------------------------\")\n","\t\tavg_testacc+=f1\n","\n","\treturn avg_testacc/runs\t\t"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L1vc3MMe5xei"},"source":["#main"]},{"cell_type":"code","execution_count":119,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T16:34:27.987884Z","iopub.status.busy":"2023-06-27T16:34:27.987469Z","iopub.status.idle":"2023-06-27T16:34:27.998039Z","shell.execute_reply":"2023-06-27T16:34:27.997214Z","shell.execute_reply.started":"2023-06-27T16:34:27.987836Z"},"id":"XUy3ItLj5yQM","trusted":true},"outputs":[],"source":["\n","import argparse\n","import random\n","import numpy as np \n","from sklearn.model_selection import *\n","from sklearn.metrics import *\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F \n","\n","\n","np.random.seed(1332)\n","torch.manual_seed(0)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Restaurant"]},{"cell_type":"code","execution_count":120,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T16:34:27.999865Z","iopub.status.busy":"2023-06-27T16:34:27.999482Z","iopub.status.idle":"2023-06-27T16:44:42.970617Z","shell.execute_reply":"2023-06-27T16:44:42.969444Z","shell.execute_reply.started":"2023-06-27T16:34:27.999829Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training for run 1 \n","Epoch 1 Train Acc 0.7431770833333332 Val Acc 0.7209378975826971 \n","Epoch 2 Train Acc 0.7431770833333332 Val Acc 0.7209378975826971 \n","Epoch 3 Train Acc 0.7431770833333332 Val Acc 0.7209378975826971 \n","Epoch 4 Train Acc 0.7431770833333332 Val Acc 0.7209378975826971 \n","Epoch 5 Train Acc 0.7441666666666666 Val Acc 0.7213911418575063 \n","Epoch 6 Train Acc 0.7421875 Val Acc 0.723657363231552 \n","Epoch 7 Train Acc 0.7411979166666666 Val Acc 0.7259235846055979 \n","Epoch 8 Train Acc 0.7421875 Val Acc 0.726376828880407 \n","Epoch 9 Train Acc 0.7461458333333333 Val Acc 0.7276232506361322 \n","Epoch 10 Train Acc 0.7461458333333333 Val Acc 0.7284164281170483 \n","Pair Score: F1 Score 0.6687294407894736 Precision 70.31831335262505 Recall 0.6546509262465374 \n","Aspect Score: F1 Score 0.7409744496282255 Precision 74.01927721328951 Recall 0.7039257271468145 \n","---------------------------------------------------\n"]}],"source":["# parser = argparse.ArgumentParser()\n","# parser.add_argument('-da','--dataset',type=str,help='dataset',default='restaurant')\n","# parser.add_argument('-ru','--runs',type=int,help='number of runs',default=5)\n","\n","# args = parser.parse_args()\n","# dataset = args.dataset\n","# runs = args.runs\n","datapath = r'/kaggle/input/dataset'       \n","embedpath = r'/kaggle/input/phoword2vec-vi-words/word2vec_vi_words_300dims.txt'\n","\n","runs =1\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","traincorpus,testcorpus,train_aspect,test_aspect,trainlabels,testlabels = load_data(datapath+\"/\")\n","\n","numclasses = max(trainlabels)+1\n","\n","trainloader,valloader,testloader,senvocab,aspvocab = generate_batches(traincorpus,testcorpus,train_aspect,test_aspect,trainlabels,testlabels)\n","\n","embedding_index,embed_dim = load_embed(embedpath)\n","\n","sentenceembed,aspectembed = load_embeddings(embedding_index,embed_dim,senvocab,aspvocab)\n","\n","gated_cnn = GatedCNN(sentenceembed,aspectembed,embed_dim,numclasses).to(device)\n","\n","test_acc = train_model(trainloader,valloader,testloader,sentenceembed,aspectembed,embed_dim,numclasses,device,runs)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
